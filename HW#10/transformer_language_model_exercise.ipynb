{"cells":[{"cell_type":"markdown","metadata":{"id":"PU0wD0LEBbVZ"},"source":["# HW10_Language Model with Transformers\n","\n","### Task   \n","\n","The goal is to learn to predict the next word from an input sequence with a language model transformer.\n","\n","The dataset is PTB.\n","\n"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive/', force_remount=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_NW60OjsHvlZ","executionInfo":{"status":"ok","timestamp":1745463162775,"user_tz":240,"elapsed":24983,"user":{"displayName":"Tarun Gowda","userId":"07863081440846510341"}},"outputId":"36ef8427-91af-452e-b383-50ba4381a4e7"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}]},{"cell_type":"code","source":["# %%\n","import sys\n","# revise the following path based on your drive\n","sys.path.append('/content/drive/MyDrive/ECE601-Module10')"],"metadata":{"id":"23tRU-fjHyg7","executionInfo":{"status":"ok","timestamp":1745463178296,"user_tz":240,"elapsed":45,"user":{"displayName":"Tarun Gowda","userId":"07863081440846510341"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":8160,"status":"ok","timestamp":1745463209588,"user":{"displayName":"Tarun Gowda","userId":"07863081440846510341"},"user_tz":240},"id":"A3yiLJBBBbVh"},"outputs":[],"source":["import torch\n","import torch.nn.functional as F\n","import torch.nn as nn\n","import math\n","import time\n","import utils"]},{"cell_type":"markdown","metadata":{"id":"Pe01LtUIBbVj"},"source":["### GPU\n","\n","It is recommended to run this code on GPU:<br>\n","* Time for 1 epoch on GPU : 48 sec w/ Google Colab Tesla P100-PCIE-16GB <br>"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25,"status":"ok","timestamp":1745463213067,"user":{"displayName":"Tarun Gowda","userId":"07863081440846510341"},"user_tz":240},"id":"g_A4HBT3BbVj","outputId":"8af99e3c-7d37-4373-c59b-1b16533a2687"},"outputs":[{"output_type":"stream","name":"stdout","text":["cuda\n","cuda available with GPU: Tesla T4\n"]}],"source":["device= torch.device(\"cuda\")\n","# device= torch.device(\"cpu\")\n","print(device)\n","\n","if torch.cuda.is_available():\n","    print('cuda available with GPU:',torch.cuda.get_device_name(0))"]},{"cell_type":"markdown","metadata":{"id":"6S3W2p84BbVk"},"source":["### Download Penn Tree Bank\n","\n","The tensor train_data consists of 20 columns of 46,479 words.<br>\n","The tensor test_data consists of 20 columns of 4,121 words."]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1815,"status":"ok","timestamp":1745463226816,"user":{"displayName":"Tarun Gowda","userId":"07863081440846510341"},"user_tz":240},"id":"TnE-JNp_BbVk","outputId":"8cc593fb-4e2f-4bbf-bbe3-18db997ee15d"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([46479, 20])\n","torch.Size([4121, 20])\n"]}],"source":["from utils import check_ptb_dataset_exists\n","data_path=check_ptb_dataset_exists()\n","\n","train_data  =  torch.load(data_path+'ptb/train_data.pt')\n","test_data   =  torch.load(data_path+'ptb/test_data.pt')\n","\n","print(  train_data.size()  )\n","print(  test_data.size()   )"]},{"cell_type":"markdown","metadata":{"id":"BDxTJimKBbVl"},"source":["### Some constants associated with the data set"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"MS9EUUvbBbVm","executionInfo":{"status":"ok","timestamp":1745463229412,"user_tz":240,"elapsed":7,"user":{"displayName":"Tarun Gowda","userId":"07863081440846510341"}}},"outputs":[],"source":["bs = 20\n","vocab_size = 10000"]},{"cell_type":"markdown","metadata":{"id":"7U9UvE28BbVm"},"source":["### Make an attention net class"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"RRmg_OUaBbVn","executionInfo":{"status":"ok","timestamp":1745463817379,"user_tz":240,"elapsed":40,"user":{"displayName":"Tarun Gowda","userId":"07863081440846510341"}}},"outputs":[],"source":["\n","def generate_positional_encoding(seq_length, dim):\n","    assert dim == 2* (dim//2) # check if dim is divisible by 2\n","    pe = torch.zeros(seq_length, dim)\n","    position = torch.arange(0, seq_length, dtype=torch.float).unsqueeze(1)\n","    div_term = torch.exp(torch.arange(0, dim, 2).float() * (-torch.log(torch.tensor(10000.0)) / dim))\n","    pe[:,0::2] = torch.sin(position * div_term)\n","    pe[:,1::2] = torch.cos(position * div_term)\n","    return pe\n","\n","class AttentionHead(nn.Module):\n","    def __init__(self, d, d_head, dropout):\n","        super().__init__()\n","        self.LN_MHA = nn.LayerNorm(d_head)\n","        self.LN_MLP = nn.LayerNorm(d_head)\n","        self.query = nn.Linear(d, d_head, bias=False) # query embedding layer\n","        self.key = nn.Linear(d, d_head, bias=False) # key embedding layer\n","        self.value = nn.Linear(d, d_head) # value embedding layer\n","        self.dropout = nn.Dropout(dropout)\n","    def forward(self, H): # size(H)=[batch_size, seq_length, d]\n","        batch_size = H.size(0); batch_len = H.size(1)\n","        # Compute a single attention head H = Softmax( QK^T / d^0.5 ) V\n","        Q = self.query(H) # COMPLETE HERE  # size=[batch_size, batch_length, d]\n","        K = self.key(H) # COMPLETE HERE  # size=[batch_size, batch_length, d]\n","        V = self.value(H) # COMPLETE HERE  # size=[batch_size, batch_length, d]\n","        attention_score = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(Q.size(-1)) # COMPLETE HERE  # QK^T/sqrt(d), (B,L,d) @ (B,d,L) => (B,L,L), size=[batch_size, batch_length, batch_length)\n","        mask = torch.tril(torch.ones(batch_len,batch_len)).long().to(device) # mask to use previous tokens only : { token(<=t) }, size=[batch_len,batch_len]\n","        attention_score = attention_score.masked_fill(mask==0, value=float('-inf')) # softmax(-inf)=0 prevents using next tokens for prediction, size=(batch_size, batch_len, batch_len)\n","        attention_score = torch.softmax(attention_score, dim=-1) # COMPLETE HERE ) # sum weights = 1, size=[batch_size, batch_length, batch_len)\n","        attention_score = self.dropout(attention_score) # dropout attention scores\n","        H_HA = torch.matmul(attention_score, V) # COMPLETE HERE  # softmax( QK^T / sqrt(d) ) V, (B,L,L) @ (B,L,d) => (B,L,d), size=[batch_size, batch_length, d)\n","        return H_HA # return prediction scores for next token\n","\n","class MultipleAttentionHead(nn.Module):\n","    def __init__(self, d, num_heads, dropout):\n","        super().__init__()\n","        d_head = d // num_heads # dim_head = d // num_heads, usually dimension per head is 64\n","        assert d == d_head * num_heads # check divisibility\n","        self.MHA = nn.ModuleList([ AttentionHead(d, d_head, dropout) for _ in range(num_heads) ])\n","        self.WO = nn.Linear(d, d) # combination layer\n","        self.dropout = nn.Dropout(dropout)\n","    def forward(self, H): # size(H)=[batch_size, seq_length, d]\n","        batch_size = H.size(0); seq_length = H.size(1)\n","        H_heads = []\n","        # COMPLETE HERE\n","        for MHA in self.MHA:\n","            H_heads.append(MHA(H))\n","\n","        H_heads = torch.cat(H_heads, dim=2) # size=[batch_size, seq_length, d]\n","        H_heads = self.dropout(H_heads) # dropout attention activations\n","        H = self.WO(H_heads) # size=[batch_size, seq_length, d]\n","        return H\n","\n","class TransformerBlock(nn.Module):\n","    def __init__(self, d, num_heads, dropout):\n","        super().__init__()\n","        self.LN_MHA = nn.LayerNorm(d)\n","        self.LN_MLP = nn.LayerNorm(d)\n","        self.MHA = MultipleAttentionHead(d, num_heads, dropout)\n","        self.MLP = nn.Sequential(nn.Linear(d,4*d), nn.ReLU(), nn.Dropout(dropout), nn.Linear(4*d,d))\n","    def forward(self, H): # size=[batch_size, seq_length, d]\n","        # Multiple Attention Heads w/ layer normalization (LN), residual connection (RC)\n","        # COMPLETE HERE\n","        H = H + self.MHA(self.LN_MHA(H))\n","\n","        # MLP w/ layer normalization (LN), residual connection (RC)\n","        # COMPLETE HERE\n","        H = H + self.MLP(self.LN_MLP(H))\n","\n","        return H # size=[batch_size, seq_length, d]\n","\n","\n","class Transformer_decoder(nn.Module):\n","    def __init__(self, d, num_heads, num_blocks, seq_length, dropout):\n","        super().__init__()\n","        self.TR_Blocks = nn.ModuleList([ TransformerBlock(d, num_heads, dropout) for _ in range(num_blocks) ])\n","    def forward(self, batch_seq, pos_enc):\n","        H = batch_seq.transpose(1,0) # size=[batch_size, seq_length, d]\n","        batch_size = H.size(0); batch_len = H.size(1)\n","        # Add positional encoding\n","        pos_enc = pos_enc.unsqueeze(dim=0) # size=[1,          seq_length, d]\n","        H = H + pos_enc # COMPLETE HERE                     # size=[batch_size, seq_length, d]\n","        # Apply transformer blocks\n","        for TR_Block in self.TR_Blocks:\n","            H = TR_Block(H)\n","        # Output\n","        H = H.permute(1,0,2)  # size=[batch_length, batch_size, d]\n","        return H # return prediction scores for next token\n","\n","\n","class ANN(nn.Module):\n","\n","    def __init__(self, d, num_heads, num_blocks, seq_length, dropout):\n","        super(ANN, self).__init__()\n","        self.decoder = Transformer_decoder(d, num_heads, num_blocks, seq_length, dropout)\n","\n","    def forward(self, g_seq , pos ):\n","        h_dec_seq = self.decoder( g_seq , pos )\n","        return h_dec_seq\n","\n","\n","class attention_net(nn.Module):\n","\n","    def __init__(self, d, num_heads, num_blocks, seq_length, dropout):\n","        super(attention_net, self).__init__()\n","        self.layer1 = nn.Embedding( vocab_size  , hidden_size  )\n","        self.layer2 = ANN(d, num_heads, num_blocks, seq_length, dropout)\n","        self.layer3 = nn.Linear(    hidden_size , vocab_size   )\n","\n","    def forward(self, word_seq, pos ):\n","        g_seq     =   self.layer1( word_seq ) # size=(seq_length, bs, hidden_dim)\n","        h_seq     =   self.layer2( g_seq , pos ) # size=(seq_length, bs, hidden_dim)\n","        score_seq =   self.layer3( h_seq ) # size=(seq_length, bs, vocab_size)\n","        return score_seq\n"]},{"cell_type":"markdown","metadata":{"id":"r4FH3SgyBbVt"},"source":["### Function to evaluate the network on the test set"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"wcOReFN0IHcP","executionInfo":{"status":"ok","timestamp":1745463822992,"user_tz":240,"elapsed":18,"user":{"displayName":"Tarun Gowda","userId":"07863081440846510341"}}},"outputs":[],"source":["def eval_on_test_set():\n","\n","    net.eval()\n","\n","    running_loss=0\n","    num_batches=0\n","\n","    for count in range( 0 , 4120-seq_length ,  seq_length) :\n","\n","        minibatch_data =  test_data[ count   : count+seq_length   ]\n","        minibatch_label = test_data[ count+1 : count+seq_length+1 ]\n","        pos = generate_positional_encoding(seq_length, hidden_size)\n","\n","        minibatch_data = minibatch_data.to(device)\n","        minibatch_label = minibatch_label.to(device)\n","        pos = pos.to(device)\n","\n","        scores = net( minibatch_data, pos )\n","\n","        minibatch_label = minibatch_label.view(  bs*seq_length )\n","        scores = scores.view(  bs*seq_length , vocab_size)\n","\n","        loss = criterion(scores, minibatch_label)\n","\n","        running_loss += loss.item()\n","        num_batches += 1\n","\n","    total_loss = running_loss/num_batches\n","    print('test: exp(loss) = ', math.exp(total_loss)  )\n"]},{"cell_type":"markdown","metadata":{"id":"4gnTKxy9BbVr"},"source":["### Build the net. Choose the hidden size to be 128, the number of heads to be 4, and the number of blocks 2.\n","### How many parameters in total?"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":87,"status":"ok","timestamp":1745463839648,"user":{"displayName":"Tarun Gowda","userId":"07863081440846510341"},"user_tz":240},"id":"ADOYeTS6_SLO","outputId":"a55f53d3-836d-4f1e-a4c6-e2092cd563e2"},"outputs":[{"output_type":"stream","name":"stdout","text":["attention_net(\n","  (layer1): Embedding(10000, 128)\n","  (layer2): ANN(\n","    (decoder): Transformer_decoder(\n","      (TR_Blocks): ModuleList(\n","        (0-1): 2 x TransformerBlock(\n","          (LN_MHA): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n","          (LN_MLP): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n","          (MHA): MultipleAttentionHead(\n","            (MHA): ModuleList(\n","              (0-3): 4 x AttentionHead(\n","                (LN_MHA): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n","                (LN_MLP): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n","                (query): Linear(in_features=128, out_features=32, bias=False)\n","                (key): Linear(in_features=128, out_features=32, bias=False)\n","                (value): Linear(in_features=128, out_features=32, bias=True)\n","                (dropout): Dropout(p=0.95, inplace=False)\n","              )\n","            )\n","            (WO): Linear(in_features=128, out_features=128, bias=True)\n","            (dropout): Dropout(p=0.95, inplace=False)\n","          )\n","          (MLP): Sequential(\n","            (0): Linear(in_features=128, out_features=512, bias=True)\n","            (1): ReLU()\n","            (2): Dropout(p=0.95, inplace=False)\n","            (3): Linear(in_features=512, out_features=128, bias=True)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (layer3): Linear(in_features=128, out_features=10000, bias=True)\n",")\n","There are 2967056 (2.97 million) parameters in this neural network\n"]}],"source":["hidden_size = 128\n","num_heads = 4\n","num_blocks = 2\n","dropout = 0.95\n","seq_length = 100\n","\n","net = attention_net(hidden_size, num_heads, num_blocks, seq_length, dropout)\n","print(net)\n","utils.display_num_param(net)"]},{"cell_type":"markdown","metadata":{"id":"khaDFxmwBbVr"},"source":["### Send the network to the GPU"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"Sp63FT_v_SLQ","executionInfo":{"status":"ok","timestamp":1745463844399,"user_tz":240,"elapsed":174,"user":{"displayName":"Tarun Gowda","userId":"07863081440846510341"}}},"outputs":[],"source":["net = net.to(device)"]},{"cell_type":"markdown","metadata":{"id":"eecX1xVaBbVs"},"source":["### Choose the loss to be the cross-entropy and the optimizer to be Adam, as well as the following important hyperparameters:\n","* initial learning rate = 0.001\n","* sequence length = 30"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"LBm4Aw_j_SLR","executionInfo":{"status":"ok","timestamp":1745463853675,"user_tz":240,"elapsed":6058,"user":{"displayName":"Tarun Gowda","userId":"07863081440846510341"}}},"outputs":[],"source":["criterion = nn.CrossEntropyLoss()\n","\n","my_lr = 0.001\n","optimizer = torch.optim.Adam(net.parameters(), lr=my_lr)\n","\n","pos = generate_positional_encoding(seq_length, hidden_size) # size=(seq_length, hidden_dim)"]},{"cell_type":"markdown","metadata":{"id":"frtpNDBLBbVs"},"source":["### Do 5 passes through the training set\n","### Observe the train perplexity and the test perplexity"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":42612,"status":"ok","timestamp":1745463909289,"user":{"displayName":"Tarun Gowda","userId":"07863081440846510341"},"user_tz":240},"id":"McQc1_xkBbVp","outputId":"a66c4c11-395e-459d-94e1-f568d47ecc33"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","epoch= 0 \t time= 8.898634195327759 \t lr= 0.001 \t exp(loss)= 685.6611456152172\n","test: exp(loss) =  366.15103424839003\n","\n","epoch= 1 \t time= 17.229893445968628 \t lr= 0.001 \t exp(loss)= 295.1850758557199\n","test: exp(loss) =  257.92763520822194\n","\n","epoch= 2 \t time= 25.96501111984253 \t lr= 0.0009090909090909091 \t exp(loss)= 207.61838670844912\n","test: exp(loss) =  216.42407336567766\n","\n","epoch= 3 \t time= 33.74486470222473 \t lr= 0.0008264462809917355 \t exp(loss)= 162.34507702395916\n","test: exp(loss) =  193.94759603029854\n","\n","epoch= 4 \t time= 42.28819036483765 \t lr= 0.0007513148009015777 \t exp(loss)= 134.65263597775692\n","test: exp(loss) =  181.8677541894197\n"]}],"source":["start=time.time()\n","for epoch in range(5):\n","\n","    # divide the learning rate by 3 except after the first epoch\n","    if epoch >= 2:\n","        optimizer.param_groups[0]['lr'] /= 1.1\n","        my_lr = optimizer.param_groups[0]['lr']\n","\n","    # set the running quantities to zero at the beginning of the epoch\n","    running_loss=0\n","    num_batches=0\n","    for count in range( 0 , 46478-seq_length ,  seq_length):\n","\n","        # Set the gradients to zeros\n","        optimizer.zero_grad()\n","\n","        # create a minibatch and the positional encoding\n","        minibatch_data = train_data[ count   : count+seq_length   ]\n","        minibatch_label = train_data[ count+1 : count+seq_length+1 ]\n","        pos = generate_positional_encoding(seq_length, hidden_size) # size=(seq_length, hidden_dim)\n","\n","        # send them to the gpu\n","        minibatch_data = minibatch_data.to(device)\n","        minibatch_label = minibatch_label.to(device)\n","        pos = pos.to(device)\n","\n","        # forward the minibatch through the net\n","        scores = net( minibatch_data, pos ) # size=(seq_length, bs, vocab_size)\n","\n","        # reshape the scores and labels to huge batch of size bs*seq_length\n","        scores = scores.view(  bs*seq_length , vocab_size) # size=(seq_length/2.bs, vocab_size)\n","        minibatch_label = minibatch_label.view(  bs*seq_length ) # size=(seq_length/2.bs, vocab_size)\n","\n","        # Compute the average of the losses of the data points in this huge batch\n","        loss = criterion(scores, minibatch_label)\n","\n","        # backward pass to compute dL/dR, dL/dV and dL/dW\n","        loss.backward()\n","\n","        # do one step of stochastic gradient descent: R=R-lr(dL/dR), V=V-lr(dL/dV), ...\n","        optimizer.step()\n","\n","        # update the running loss\n","        running_loss += loss.item()\n","        num_batches += 1\n","\n","    # compute stats for the full training set\n","    total_loss = running_loss/num_batches\n","    elapsed = time.time()-start\n","\n","    print('')\n","    print('epoch=',epoch, '\\t time=', elapsed,'\\t lr=', my_lr, '\\t exp(loss)=',  math.exp(total_loss))\n","    eval_on_test_set()\n"]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"JRu41IdwBbVt","jupyter":{"outputs_hidden":true}},"source":["### Choose one sentence (taken from the test set)"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"gSmC_QyMBbVt","executionInfo":{"status":"ok","timestamp":1745463914910,"user_tz":240,"elapsed":6,"user":{"displayName":"Tarun Gowda","userId":"07863081440846510341"}}},"outputs":[],"source":["sentence1 = \"some analysts expect oil prices to remain relatively\"\n","\n","sentence2 = \"over the next days and weeks they say investors should look for stocks to\"\n","\n","sentence3 = \"prices averaging roughly $ N a barrel higher in the third\"\n","\n","sentence4 = \"i think my line has been very consistent mrs. hills said at a news\"\n","\n","sentence5 = \"this appears particularly true at gm which had strong sales in\"\n","\n","# or make your own sentence.  No capital letter or punctuation allowed. Each word must be in the allowed vocabulary.\n","sentence6 = \"he was very\"\n","\n","# SELECT THE SENTENCE HERE\n","mysentence = sentence3"]},{"cell_type":"markdown","metadata":{"id":"SoC9H2zBBbVu"},"source":["### Display the the network prediction for the next word"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":108,"status":"ok","timestamp":1745463925602,"user":{"displayName":"Tarun Gowda","userId":"07863081440846510341"},"user_tz":240},"id":"YenIJTXcBbVu","outputId":"bd43c914-61fe-464f-e3be-76e1b3b2d559"},"outputs":[{"output_type":"stream","name":"stdout","text":["prices averaging roughly $ N a barrel higher in the third ... \n","\n","83.5%\t quarter\n","1.7%\t period\n","1.7%\t consecutive\n","1.5%\t world\n","1.2%\t <eos>\n","0.8%\t market\n","0.5%\t level\n","0.5%\t of\n","0.3%\t session\n","0.3%\t <unk>\n","0.3%\t parties\n","0.2%\t quarters\n","0.2%\t floor\n","0.2%\t day\n","0.1%\t and\n","0.1%\t category\n","0.1%\t reich\n","0.1%\t rate\n","0.1%\t index\n","0.1%\t year\n","0.1%\t system\n","0.1%\t game\n","0.1%\t week\n","0.1%\t stock\n","0.1%\t month\n","0.1%\t account\n","0.1%\t summit\n","0.1%\t in\n","0.1%\t loans\n","0.1%\t monday\n"]}],"source":["minibatch_data = utils.sentence2vector(mysentence)\n","minibatch_data = torch.cat((minibatch_data, minibatch_data), dim=0) # copy-paste the test sequence to use the same attention window size for each word\n","pos = generate_positional_encoding(minibatch_data.size(0), hidden_size)\n","\n","minibatch_data = minibatch_data.to(device)\n","pos = pos.to(device)\n","\n","net.eval()\n","scores = net( minibatch_data, pos )\n","scores = scores[-1,:] # select the last score vector for the prediction of the next word from the input sequence\n","scores = scores[0].unsqueeze(0).unsqueeze(0)\n","\n","print(mysentence, '... \\n')\n","utils.show_next_word(scores)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-g3t66JccXCq"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bUFyPiOKGjgj"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.20"}},"nbformat":4,"nbformat_minor":0}